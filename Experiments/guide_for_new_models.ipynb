{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "9c910625",
   "metadata": {},
   "source": [
    "# EarthquakeNPP quickstart for adding a new model\n",
    "This notebook walks through the minimum steps to plug a custom model into the EarthquakeNPP benchmark:\n",
    "\n",
    "- load one of the provided catalogs\n",
    "- split events into train/val/test using the benchmark time splits\n",
    "- build sliding-window datasets that expose fixed-length histories\n",
    "- fit a tiny spatio-temporal conditional intensity function (CIF)\n",
    "- evaluate the CIF on the test split with a log-likelihood that uses numerical integration for the temporal survival term\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2155293c",
   "metadata": {},
   "source": [
    "## Set up paths and choose a dataset\n",
    "Pick any catalog listed in `CATALOG_PATHS` (they match the benchmark names in the paper). The split dates come from `Datasets/README.md`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "338ddbe3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using catalog: EarthquakeNPP/Datasets/ComCat/ComCat_catalog.csv\n",
      "Splits: {'train_start': Timestamp('1981-01-01 00:00:00'), 'val_start': Timestamp('1998-01-01 00:00:00'), 'test_start': Timestamp('2007-01-01 00:00:00'), 'test_end': Timestamp('2020-01-17 00:00:00')}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/anaconda3/envs/earthquakeNPP/lib/python3.11/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "device(type='cuda')"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from pathlib import Path\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import torch\n",
    "from torch.utils.data import DataLoader\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.distributions import Normal\n",
    "import sys\n",
    "\n",
    "# Relative paths from this notebook (located in Experiments/)\n",
    "PROJECT_ROOT = Path('..').resolve()\n",
    "AUTO_STPP_SRC = PROJECT_ROOT / 'Experiments' / 'AutoSTPP' / 'src'\n",
    "sys.path.append(str(AUTO_STPP_SRC))\n",
    "from data.data import SlidingWindowWrapper\n",
    "\n",
    "# Choose your catalog here\n",
    "DATASET = 'ComCat_25'  # e.g. ComCat_25 | SCEDC_25 | SCEDC_20 | SCEDC_30 | SanJac_10 | SaltonSea_10 | WHITE_06\n",
    "\n",
    "CATALOG_PATHS = {\n",
    "    'ComCat_25': PROJECT_ROOT / 'Datasets' / 'ComCat' / 'ComCat_catalog.csv',\n",
    "    'SCEDC_20': PROJECT_ROOT / 'Datasets' / 'SCEDC' / 'SCEDC_catalog.csv',\n",
    "    'SCEDC_25': PROJECT_ROOT / 'Datasets' / 'SCEDC' / 'SCEDC_catalog.csv',\n",
    "    'SCEDC_30': PROJECT_ROOT / 'Datasets' / 'SCEDC' / 'SCEDC_catalog.csv',\n",
    "    'SanJac_10': PROJECT_ROOT / 'Datasets' / 'QTM' / 'SanJac_catalog.csv',\n",
    "    'SaltonSea_10': PROJECT_ROOT / 'Datasets' / 'QTM' / 'SaltonSea_catalog.csv',\n",
    "    'WHITE_06': PROJECT_ROOT / 'Datasets' / 'WHITE' / 'WHITE_catalog.csv',\n",
    "    'ETAS_25': PROJECT_ROOT / 'Datasets' / 'ETAS' / 'ETAS_California_catalog.csv',\n",
    "    'ETAS_incomplete_25': PROJECT_ROOT / 'Datasets' / 'ETAS' / 'ETAS_California_incomplete_catalog.csv',\n",
    "    'Japan_Deprecated': PROJECT_ROOT / 'Datasets' / 'Japan_Deprecated' / 'Japan_catalog.csv',\n",
    "}\n",
    "\n",
    "# Magnitude thresholds\n",
    "MAG_THRESHOLDS = {\n",
    "    'ComCat_25': 2.5,\n",
    "    'SCEDC_20': 2.0,\n",
    "    'SCEDC_25': 2.5,\n",
    "    'SCEDC_30': 3.0,\n",
    "    'SanJac_10': 1.0,\n",
    "    'SaltonSea_10': 1.0,\n",
    "    'WHITE_06': 0.6,\n",
    "    'ETAS_25': 1.0,\n",
    "    'ETAS_incomplete_25': 1.0,\n",
    "    'Japan_Deprecated': 2.5,\n",
    "}\n",
    "\n",
    "# Time-based splits from Datasets/README.md\n",
    "SPLIT_BOUNDARIES = {\n",
    "    'ComCat_25': dict(train_start='1981-01-01', val_start='1998-01-01', test_start='2007-01-01', test_end='2020-01-17'),\n",
    "    'SCEDC_20': dict(train_start='1985-01-01', val_start='2005-01-01', test_start='2014-01-01', test_end='2020-01-01'),\n",
    "    'SCEDC_25': dict(train_start='1985-01-01', val_start='2005-01-01', test_start='2014-01-01', test_end='2020-01-01'),\n",
    "    'SCEDC_30': dict(train_start='1985-01-01', val_start='2005-01-01', test_start='2014-01-01', test_end='2020-01-01'),\n",
    "    'SanJac_10': dict(train_start='2009-01-01', val_start='2014-01-01', test_start='2016-01-01', test_end='2018-01-01'),\n",
    "    'SaltonSea_10': dict(train_start='2009-01-01', val_start='2014-01-01', test_start='2016-01-01', test_end='2018-01-01'),\n",
    "    'WHITE_06': dict(train_start='2009-01-01', val_start='2014-01-01', test_start='2017-01-01', test_end='2021-01-01'),\n",
    "    'ETAS_25': dict(train_start='1981-01-01', val_start='1998-01-01', test_start='2007-01-01', test_end='2020-01-17'),\n",
    "    'ETAS_incomplete_25': dict(train_start='1981-01-01', val_start='1998-01-01', test_start='2007-01-01', test_end='2020-01-17'),\n",
    "    'Japan_Deprecated': dict(train_start='1992-01-01', val_start='2007-01-01', test_start='2011-01-01', test_end='2020-01-01'),\n",
    "}\n",
    "\n",
    "catalog_path = CATALOG_PATHS[DATASET]\n",
    "split_dates = {k: pd.Timestamp(v) for k, v in SPLIT_BOUNDARIES[DATASET].items()}\n",
    "print(f'Using catalog: {catalog_path}')\n",
    "print(f\"Splits: {split_dates}\")\n",
    "\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "device\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9532fcd9",
   "metadata": {},
   "source": [
    "## Load the catalog and create the benchmark splits\n",
    "We keep only the `time`, `x`, and `y` columns that all baselines consume. Times are converted to days since the first event in each split so that magnitudes of the inputs stay reasonable."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "83e099ef",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                     time           x           y\n",
      "0 1971-01-01 20:36:17.720 -221.828430  -52.446502\n",
      "1 1971-01-02 02:19:13.010  -18.087790  112.702064\n",
      "2 1971-01-02 02:37:49.820  -14.801382  114.585400\n",
      "3 1971-01-02 06:27:39.120   -5.390044 -127.740486\n",
      "4 1971-01-02 07:59:08.050  -24.172068  113.773295\n",
      "Train events: 40,701\n",
      "Val events:   14,741\n",
      "Test events:  21,885\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "((40701, 3), (14741, 3), (21885, 3))"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.read_csv(catalog_path, parse_dates=['time']).sort_values('time').reset_index(drop=True)\n",
    "\n",
    "mag_filter = MAG_THRESHOLDS.get(DATASET)\n",
    "if mag_filter is not None and 'magnitude' in df.columns:\n",
    "    df = df[df['magnitude'] >= mag_filter].reset_index(drop=True)\n",
    "\n",
    "print(df[['time', 'x', 'y']].head())\n",
    "\n",
    "train_df = df[(df.time >= split_dates['train_start']) & (df.time < split_dates['val_start'])]\n",
    "val_df = df[(df.time >= split_dates['val_start']) & (df.time < split_dates['test_start'])]\n",
    "test_df = df[(df.time >= split_dates['test_start']) & (df.time < split_dates['test_end'])]\n",
    "\n",
    "print(f\"Train events: {len(train_df):,}\")\n",
    "print(f\"Val events:   {len(val_df):,}\")\n",
    "print(f\"Test events:  {len(test_df):,}\")\n",
    "\n",
    "# Convert each split to a single continuous sequence [time, x, y], time measured in days from the split start\n",
    "def df_to_sequence(split_df):\n",
    "    if len(split_df) == 0:\n",
    "        raise ValueError('Split is empty. Check split boundaries for the chosen dataset.')\n",
    "    t0 = split_df['time'].iloc[0]\n",
    "    t_days = (split_df['time'] - t0).dt.total_seconds() / 86400.0\n",
    "    return np.stack([t_days.to_numpy(), split_df['x'].to_numpy(), split_df['y'].to_numpy()], axis=1)\n",
    "\n",
    "train_seq = df_to_sequence(train_df)\n",
    "val_seq = df_to_sequence(val_df)\n",
    "test_seq = df_to_sequence(test_df)\n",
    "\n",
    "train_seq.shape, val_seq.shape, test_seq.shape\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1c52409c",
   "metadata": {},
   "source": [
    "## Build sliding-window datasets\n",
    "`SlidingWindowWrapper` converts the single long sequence into fixed-length histories (`st_X`) and their next-event targets (`st_Y`). Setting `roll=False` keeps time as the first column, which is convenient for the log-likelihood below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "14c5fc84",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sliding windows: train=40,669, val=14,709, test=21,853\n",
      "Example history shape: torch.Size([32, 3])\n",
      "Example next event (absolute time, x, y): tensor([[  4.9511, 268.8405,   8.6861]], device='cuda:0')\n"
     ]
    }
   ],
   "source": [
    "LOOKBACK = 32   # number of past events per window\n",
    "LOOKAHEAD = 1  # predict only the next event for simplicity\n",
    "\n",
    "train_ds = SlidingWindowWrapper([train_seq], lookback=LOOKBACK, lookahead=LOOKAHEAD, roll=False, normalized=False, device=device)\n",
    "val_ds = SlidingWindowWrapper([val_seq], lookback=LOOKBACK, lookahead=LOOKAHEAD, roll=False, normalized=False, device=device)\n",
    "test_ds = SlidingWindowWrapper([test_seq], lookback=LOOKBACK, lookahead=LOOKAHEAD, roll=False, normalized=False, device=device)\n",
    "\n",
    "train_loader = DataLoader(train_ds, batch_size=256, shuffle=True)\n",
    "val_loader = DataLoader(val_ds, batch_size=256)\n",
    "test_loader = DataLoader(test_ds, batch_size=256)\n",
    "\n",
    "print(f\"Sliding windows: train={len(train_ds):,}, val={len(val_ds):,}, test={len(test_ds):,}\")\n",
    "first_x, first_y, first_x_cum, first_y_cum, idx = train_ds[0]\n",
    "print('Example history shape:', first_x.shape)\n",
    "print('Example next event (absolute time, x, y):', first_y_cum)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3b384f39",
   "metadata": {},
   "source": [
    "## Define a spatio-temporal conditional intensity\n",
    "We factor the CIF into a **temporal rate** and a **spatial density**:\n",
    "\n",
    "- `lambda_time(t | history)` is non-negative and only affects the survival integral. We integrate it over time; assuming the spatial domain is infinite means the integral over space of the spatial density is 1.\n",
    "- `p(x, y | history)` is a normalized Gaussian density over the infinite plane, so its integral over space is 1. The log-probability contributes to the event log-likelihood.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "265ad632",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DummySpatioTemporalCIF(\n",
       "  (encoder): GRU(3, 64, batch_first=True)\n",
       "  (time_head): Sequential(\n",
       "    (0): Linear(in_features=66, out_features=64, bias=True)\n",
       "    (1): ReLU()\n",
       "    (2): Linear(in_features=64, out_features=1, bias=True)\n",
       "  )\n",
       "  (spatial_head): Sequential(\n",
       "    (0): Linear(in_features=64, out_features=64, bias=True)\n",
       "    (1): ReLU()\n",
       "    (2): Linear(in_features=64, out_features=4, bias=True)\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "class DummySpatioTemporalCIF(nn.Module):\n",
    "    def __init__(self, input_dim=3, hidden_dim=64):\n",
    "        super().__init__()\n",
    "        self.encoder = nn.GRU(input_dim, hidden_dim, batch_first=True)\n",
    "        self.time_head = nn.Sequential(\n",
    "            nn.Linear(hidden_dim + 2, hidden_dim),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(hidden_dim, 1),\n",
    "        )\n",
    "        self.spatial_head = nn.Sequential(\n",
    "            nn.Linear(hidden_dim, hidden_dim),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(hidden_dim, 4),  # mean_x, mean_y, log_std_x, log_std_y\n",
    "        )\n",
    "\n",
    "    def encode_history(self, history):\n",
    "        # history: [B, lookback, input_dim]\n",
    "        _, h = self.encoder(history)\n",
    "        return h[-1]  # [B, hidden_dim]\n",
    "\n",
    "    def lambda_time(self, history, t_eval):\n",
    "        \"\"\"\n",
    "        history: [B, lookback, 3]\n",
    "        t_eval: [B, K] times (days) measured from the last history event\n",
    "        returns: [B, K] temporal intensities (non-negative)\n",
    "        \"\"\"\n",
    "        h = self.encode_history(history)\n",
    "        t_features = torch.stack([t_eval, t_eval ** 2], dim=-1)\n",
    "        h_expanded = h.unsqueeze(1).expand(-1, t_eval.shape[1], -1)\n",
    "        logits = self.time_head(torch.cat([t_features, h_expanded], dim=-1)).squeeze(-1)\n",
    "        return F.softplus(logits) + 1e-6\n",
    "\n",
    "    def spatial_distribution(self, history):\n",
    "        \"\"\"\n",
    "        Returns a factorized Gaussian over (x, y) with parameters conditioned on history.\n",
    "        Integral over the infinite plane equals 1, matching the assumption of infinite spatial domain.\n",
    "        \"\"\"\n",
    "        h = self.encode_history(history)\n",
    "        params = self.spatial_head(h)\n",
    "        mean = params[:, :2]\n",
    "        log_std = params[:, 2:].clamp(-5, 5)  # keep std numerically sane\n",
    "        std = torch.exp(log_std)\n",
    "        dist_x = Normal(mean[:, 0], std[:, 0])\n",
    "        dist_y = Normal(mean[:, 1], std[:, 1])\n",
    "        return dist_x, dist_y\n",
    "\n",
    "model = DummySpatioTemporalCIF().to(device)\n",
    "model\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c909efb6",
   "metadata": {},
   "source": [
    "## Log-likelihood with temporal integration and spatial density\n",
    "For each window the log-likelihood is split into temporal and spatial parts:\n",
    "\n",
    "$$ \\underbrace{\\log \\lambda_\\text{time}(t_i) - \\int_0^T \\lambda_\\text{time}(t) \\, dt}_{\\text{temporal}} + \\underbrace{\\log p(x_i, y_i)}_{\\text{spatial}} $$\n",
    "\n",
    "The spatial density integrates to 1 over the infinite plane, so only the temporal part appears in the survival integral."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "2726675a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor(-0.6562, device='cuda:0', grad_fn=<MeanBackward0>),\n",
       " tensor(-84648.4062, device='cuda:0', grad_fn=<MeanBackward0>),\n",
       " tensor(-84649.0625, device='cuda:0', grad_fn=<MeanBackward0>))"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def loglik_batch(model, batch, device, integration_steps=128):\n",
    "    \"\"\"Returns temporal, spatial, and total log-likelihood means for a batch.\"\"\"\n",
    "    st_x, st_y, st_x_cum, st_y_cum, _ = batch\n",
    "    history = st_x.to(device)\n",
    "    history_abs = st_x_cum.to(device)\n",
    "    future_abs = st_y_cum.to(device)\n",
    "\n",
    "    last_history_time = history_abs[:, -1, 0]\n",
    "    next_event_time = future_abs[:, :, 0].squeeze(-1)  # [B]\n",
    "    event_xy = future_abs[:, 0, 1:3]  # [B, 2]\n",
    "\n",
    "    horizon = torch.clamp(next_event_time - last_history_time, min=1e-5)\n",
    "\n",
    "    base_grid = torch.linspace(0.0, 1.0, steps=integration_steps, device=device)\n",
    "    t_grid = horizon.unsqueeze(1) * base_grid  # [B, K]\n",
    "\n",
    "    lambda_grid = model.lambda_time(history, t_grid)\n",
    "    integral = torch.trapz(lambda_grid, t_grid, dim=1)\n",
    "\n",
    "    event_lambda = model.lambda_time(history, horizon.unsqueeze(1)).squeeze(-1)\n",
    "\n",
    "    dist_x, dist_y = model.spatial_distribution(history)\n",
    "    spatial_logprob = dist_x.log_prob(event_xy[:, 0]) + dist_y.log_prob(event_xy[:, 1])\n",
    "\n",
    "    temporal_ll = torch.log(event_lambda + 1e-8) - integral\n",
    "    total_ll = temporal_ll + spatial_logprob\n",
    "\n",
    "    temporal_mean = temporal_ll.mean()\n",
    "    spatial_mean = spatial_logprob.mean()\n",
    "    total_mean = total_ll.mean()\n",
    "    return temporal_mean, spatial_mean, total_mean\n",
    "\n",
    "# quick smoke test\n",
    "loglik_batch(model, next(iter(train_loader)), device)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5144e135",
   "metadata": {},
   "source": [
    "## Train briefly\n",
    "We minimize the negative total log-likelihood but track temporal and spatial parts separately."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "f4f2e15c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1: train temporal=0.2912, spatial=-10581.2646, total=-10580.9733\n",
      "          val temporal=0.3943, spatial=-804.5598, total=-804.1655\n",
      "Epoch 2: train temporal=0.8133, spatial=-377.7007, total=-376.8874\n",
      "          val temporal=0.3739, spatial=-206.8791, total=-206.5052\n",
      "Epoch 3: train temporal=0.8614, spatial=-110.8762, total=-110.0149\n",
      "          val temporal=0.3950, spatial=-95.0211, total=-94.6260\n",
      "Epoch 4: train temporal=0.8632, spatial=-58.2378, total=-57.3746\n",
      "          val temporal=0.4065, spatial=-59.6350, total=-59.2284\n",
      "Epoch 5: train temporal=0.8774, spatial=-37.9387, total=-37.0612\n",
      "          val temporal=0.4291, spatial=-44.5775, total=-44.1484\n",
      "Epoch 6: train temporal=0.8968, spatial=-29.1259, total=-28.2290\n",
      "          val temporal=0.4448, spatial=-36.9842, total=-36.5395\n",
      "Epoch 7: train temporal=0.9218, spatial=-25.2101, total=-24.2883\n",
      "          val temporal=0.4653, spatial=-32.2704, total=-31.8051\n",
      "Epoch 8: train temporal=0.9518, spatial=-22.1353, total=-21.1835\n",
      "          val temporal=0.4828, spatial=-29.1188, total=-28.6360\n",
      "Epoch 9: train temporal=0.9747, spatial=-20.2499, total=-19.2752\n",
      "          val temporal=0.5051, spatial=-26.8859, total=-26.3808\n",
      "Epoch 10: train temporal=0.9792, spatial=-18.9780, total=-17.9988\n",
      "          val temporal=0.5124, spatial=-25.3026, total=-24.7901\n"
     ]
    }
   ],
   "source": [
    "EPOCHS = 10\n",
    "MAX_TRAIN_BATCHES = 50\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=1e-3)\n",
    "\n",
    "for epoch in range(1, EPOCHS + 1):\n",
    "    model.train()\n",
    "    t_logs, s_logs, tot_logs = [], [], []\n",
    "    for step, batch in enumerate(train_loader, start=1):\n",
    "        optimizer.zero_grad()\n",
    "        temporal_ll, spatial_ll, total_ll = loglik_batch(model, batch, device)\n",
    "        loss = -total_ll\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        t_logs.append(float(temporal_ll.item()))\n",
    "        s_logs.append(float(spatial_ll.item()))\n",
    "        tot_logs.append(float(total_ll.item()))\n",
    "        if step >= MAX_TRAIN_BATCHES:\n",
    "            break\n",
    "    print(f\"Epoch {epoch}: train temporal={np.mean(t_logs):.4f}, spatial={np.mean(s_logs):.4f}, total={np.mean(tot_logs):.4f}\")\n",
    "\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        vt_logs, vs_logs, vtot_logs = [], [], []\n",
    "        for batch in val_loader:\n",
    "            temporal_ll, spatial_ll, total_ll = loglik_batch(model, batch, device)\n",
    "            vt_logs.append(float(temporal_ll.item()))\n",
    "            vs_logs.append(float(spatial_ll.item()))\n",
    "            vtot_logs.append(float(total_ll.item()))\n",
    "    print(f\"          val temporal={np.mean(vt_logs):.4f}, spatial={np.mean(vs_logs):.4f}, total={np.mean(vtot_logs):.4f}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "09187003",
   "metadata": {},
   "source": [
    "## Evaluate on the test split\n",
    "We report temporal, spatial, and total log-likelihood means."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "19c5b75c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test temporal log-likelihood: 0.6687\n",
      "Test spatial log-likelihood:  -35.4902\n",
      "Test total log-likelihood:    -34.8215\n"
     ]
    }
   ],
   "source": [
    "model.eval()\n",
    "with torch.no_grad():\n",
    "    tt_logs, ts_logs, ttot_logs = [], [], []\n",
    "    for batch in test_loader:\n",
    "        temporal_ll, spatial_ll, total_ll = loglik_batch(model, batch, device, integration_steps=256)\n",
    "        tt_logs.append(float(temporal_ll.item()))\n",
    "        ts_logs.append(float(spatial_ll.item()))\n",
    "        ttot_logs.append(float(total_ll.item()))\n",
    "\n",
    "print(f\"Test temporal log-likelihood: {np.mean(tt_logs):.4f}\")\n",
    "print(f\"Test spatial log-likelihood:  {np.mean(ts_logs):.4f}\")\n",
    "print(f\"Test total log-likelihood:    {np.mean(ttot_logs):.4f}\")\n"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
